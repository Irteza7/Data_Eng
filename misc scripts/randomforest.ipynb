{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc7bdbd9-4527-4b50-b8eb-9b4e0ab9564d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark2pmml in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (0.5.1)\n",
      "Requirement already satisfied: py4j in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from pyspark2pmml) (0.10.7)\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "!pip3 install pyspark2pmml \n",
    "from pyspark2pmml import PMMLBuilder\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "import logging\n",
    "import shutil\n",
    "import site\n",
    "import sys\n",
    "import wget\n",
    "import re\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8029366-c0dd-4500-87c8-4505cd1947e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_csv = os.environ.get('data_csv', 'data.csv')\n",
    "data_parquet = os.environ.get('data_parquet', 'data.parquet')\n",
    "master = os.environ.get('master', \"local[*]\")\n",
    "data_dir = os.environ.get('data_dir', '../../data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ab804e2-753f-478e-94d8-a540ed344074",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_parquet = 'data.parquet'\n",
    "data_csv = 'trends.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fe51e0a-7025-450d-8d88-c18f4bd67511",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "skip = False\n",
    "if os.path.exists(data_dir + data_csv):\n",
    "    skip = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "510c7122-1172-4017-a06c-653e0cefd849",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip:\n",
    "    sc = SparkContext.getOrCreate(SparkConf().setMaster(master))\n",
    "    spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "199f0f01-340f-4625-ac45-9a8e97ea0135",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "if not skip:\n",
    "    df = spark.read.parquet(data_dir + data_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6801b228-5692-4539-8323-8247b8992d3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "if not skip:\n",
    "    if os.path.exists(data_dir + data_csv):\n",
    "        shutil.rmtree(data_dir + data_csv)\n",
    "    df.coalesce(1).write.option(\"header\", \"true\").csv(data_dir + data_csv)\n",
    "    file = glob.glob(data_dir + data_csv + '/part-*')\n",
    "    shutil.move(file[0], data_dir + data_csv + '.tmp')\n",
    "    shutil.rmtree(data_dir + data_csv)\n",
    "    shutil.move(data_dir + data_csv + '.tmp', data_dir + data_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a600f02d-0cac-4e19-9686-4829643d93e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_csv = os.environ.get('trends_csv',\n",
    "                              'trends.csv')  # input file name (parquet)\n",
    "master = os.environ.get('master',\n",
    "                        \"local[*]\")  # URL to Spark master\n",
    "model_target = os.environ.get('model_target',\n",
    "                              \"model.xml\")  # model output file name\n",
    "data_dir = os.environ.get('data_dir',\n",
    "                          '../../data/')  # temporary directory for data\n",
    "input_columns = os.environ.get('input_columns',\n",
    "                               '[\"x\", \"y\", \"z\"]')  # input columns to consider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "187a453a-630e-4058-b809-664a1a35b12f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conf = SparkConf().setMaster(master)\n",
    "#if sys.version[0:3] == '3.6' or sys.version[0:3] == '3.7':\n",
    "conf.set(\"spark.jars\", 'jpmml-sparkml-executable-1.5.12.jar')\n",
    "\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "spark = sqlContext.sparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "93214e49-9d4f-4b02-b8b2-1a0cb1620298",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(data_dir + data_csv, header=True, inferSchema=True)\n",
    "df.createOrReplaceTempView('df')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bd483158-9f74-4388-a45f-1387bdfffec0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "df = df.withColumn(\"x\", df.x.cast(DoubleType()))\n",
    "df = df.withColumn(\"y\", df.y.cast(DoubleType()))\n",
    "df = df.withColumn(\"z\", df.z.cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "182b8c56-3757-4a28-a0fc-62580d0aee6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: numTrees=10, maxDepth=7\n",
      "Accuracy: 0.464623512687628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: numTrees=20, maxDepth=5\n",
      "Accuracy: 0.4459978284959536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 148:=================================================>       (7 + 1) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: numTrees=20, maxDepth=7\n",
      "Accuracy: 0.46748900256327025\n",
      "\n",
      "Best Hyperparameters:\n",
      "numTrees=20, maxDepth=7\n",
      "Highest Accuracy: 0.46748900256327025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Set the seed value\n",
    "seed_value = 1\n",
    "\n",
    "# Create an 80-20 training and test split\n",
    "df_train, df_test = df.randomSplit([0.8, 0.2], seed=seed_value)\n",
    "\n",
    "\n",
    "numTrees_values = [10, 20]\n",
    "maxDepth_values = [5, 7]\n",
    "\n",
    "\n",
    "\n",
    "# Define the input columns for the VectorAssembler\n",
    "input_columns = [\"x\", \"y\", \"z\"]\n",
    "\n",
    "# Create the StringIndexer for the \"class\" column\n",
    "indexer = StringIndexer(inputCol=\"class\", outputCol=\"label\")\n",
    "\n",
    "# Create the VectorAssembler\n",
    "vectorAssembler = VectorAssembler(inputCols=input_columns, outputCol=\"features\")\n",
    "\n",
    "# Create the MinMaxScaler for normalization\n",
    "normalizer = MinMaxScaler(inputCol=\"features\", outputCol=\"features_norm\")\n",
    "\n",
    "#Loop over the hyperparameter combinations\n",
    "for numTrees in numTrees_values:\n",
    "    for maxDepth in maxDepth_values:\n",
    "        # Create the RandomForestClassifier with the current hyperparameters\n",
    "        rf = RandomForestClassifier(numTrees=numTrees, maxDepth=maxDepth, seed=seed_value, labelCol=\"label\")\n",
    "\n",
    "        # Create the pipeline with the vector assembler, normalizer, and random forest classifier\n",
    "        pipeline = Pipeline(stages=[indexer, vectorAssembler, normalizer, rf])\n",
    "\n",
    "        # Fit the pipeline to the training data\n",
    "        model = pipeline.fit(df_train)\n",
    "\n",
    "        # Make predictions on the test data\n",
    "        predictions = model.transform(df_test)\n",
    "\n",
    "        # Evaluate the accuracy of the predictions\n",
    "        evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\", labelCol=\"label\")\n",
    "        accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "        # Print the accuracy for the current hyperparameters\n",
    "        print(f\"Hyperparameters: numTrees={numTrees}, maxDepth={maxDepth}\")\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "        # Check if the current hyperparameters yield the highest accuracy so far\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_hyperparameters = (numTrees, maxDepth)\n",
    "\n",
    "# Print the combination of hyperparameters that yielded the highest accuracy\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "print(f\"numTrees={best_hyperparameters[0]}, maxDepth={best_hyperparameters[1]}\")\n",
    "print(f\"Highest Accuracy: {best_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d494e50a-49f6-48a3-a798-9ce2b241123b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
